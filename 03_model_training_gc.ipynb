{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1O0QM5S00Z9xbhmEeB5eJHV5zBl1RTbUE","authorship_tag":"ABX9TyOpBvDizTaofcABCpx/xldU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Imports and configuration"],"metadata":{"id":"muC0tWfCLeVJ"}},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision.models import vgg16\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import random\n","import cv2\n","from google.colab import drive\n","import matplotlib.pyplot as plt # For plotting results\n","\n","# --- Configuration (UPDATED for Colab GPU and Part A) ---\n","# NOTE: Update this path to where your 'outputs' folder is located in Google Drive\n","COLAB_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks\")\n","OUT_DIR = COLAB_DIR / \"outputs\"\n","SAVE_TENSORS_DIR = \"tensors\"\n","\n","# --- Critical Path Fix Configuration ---\n","# Old root path found in your manifest (from your Windows desktop)\n","OLD_ROOT = r\"C:\\Users\\ACEPC\\Desktop\\DeepVision Crowd Monitoring\"\n","# New root path in Colab\n","NEW_ROOT = str(COLAB_DIR)\n","\n","# --- Hyperparameters (Optimized for GPU) ---\n","BATCH_SIZE = 10 # Optimized for GPU efficiency\n","LEARNING_RATE = 1e-6\n","NUM_EPOCHS = 50\n","NUM_WORKERS = 4 # Optimized workers for faster GPU data loading\n","LOG_BATCH_INTERVAL = 50 # For verbose batch loss printing\n","\n","# --- PyTorch Setup ---\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# --- Load Manifest File ---\n","MANIFEST_PATH = OUT_DIR / \"preprocess_manifest.csv\"\n","if not MANIFEST_PATH.exists():\n","    raise FileNotFoundError(f\"Manifest not found. Ensure Drive is mounted and data is uploaded.\")\n","manifest_df = pd.read_csv(MANIFEST_PATH)\n","TENSORS_BASE = OUT_DIR / SAVE_TENSORS_DIR\n","USE_SAVED_TENSORS = True\n","\n","# --- CRITICAL FIX 1: Replace Windows root with Colab root ---\n","manifest_df['img_path'] = manifest_df['img_path'].str.replace(OLD_ROOT, NEW_ROOT, regex=False)\n","manifest_df['density_path'] = manifest_df['density_path'].str.replace(OLD_ROOT, NEW_ROOT, regex=False)\n","manifest_df['density_full_path'] = manifest_df['density_full_path'].str.replace(OLD_ROOT, NEW_ROOT, regex=False)\n","\n","# --- CRITICAL FIX 2: Replace all remaining Windows backslashes with forward slashes ---\n","manifest_df['img_path'] = manifest_df['img_path'].str.replace('\\\\', '/', regex=False)\n","manifest_df['density_path'] = manifest_df['density_path'].str.replace('\\\\', '/', regex=False)\n","manifest_df['density_full_path'] = manifest_df['density_full_path'].str.replace('\\\\', '/', regex=False)\n","\n","\n","# --- Filter for Part A Only ---\n","# This ensures we only work with the Part A data (300 train, 182 test)\n","manifest_df = manifest_df[manifest_df['part'] == 'part_A'].reset_index(drop=True)\n","\n","print(\"✅ Manifest paths successfully cleaned and updated for Colab.\")\n","print(f\"Manifest loaded and filtered. Total Part A samples: {len(manifest_df)}.\")"],"metadata":{"id":"_jW2FtniLxPZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764166474770,"user_tz":-330,"elapsed":106,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}},"outputId":"fcfd6f38-1df6-449b-beb4-d6f035f566ed"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda:0\n","✅ Manifest paths successfully cleaned and updated for Colab.\n","Manifest loaded and filtered. Total Part A samples: 482.\n"]}]},{"cell_type":"markdown","source":["Dataset and DataLoader Utilities\n","\n","This cell contains the CrowdDatasetTorch class and the make_dataloader function, which are essential for feeding data into your model."],"metadata":{"id":"csgX8Q6xqq4c"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","# --- ImageNet Statistics (must be same as  Preprocessing Notebook) ---\n","IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","class CrowdDatasetTorch(Dataset):\n","    def __init__(self, manifest_df, use_full_density=False, transform=None, use_saved_tensors=True, tensors_base=None):\n","        self.df = manifest_df.reset_index(drop=True)\n","        self.use_full_density = use_full_density\n","        self.transform = transform\n","        self.use_saved_tensors = use_saved_tensors\n","        self.tensors_base = Path(tensors_base) if tensors_base is not None else None\n","        self.mean_t = torch.from_numpy(IMAGENET_MEAN).view(3,1,1)\n","        self.std_t = torch.from_numpy(IMAGENET_STD).view(3,1,1)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def _load_image_tensor_from_file(self, img_path):\n","        img_bgr = cv2.imread(img_path)\n","        if img_bgr is None:\n","            raise FileNotFoundError(f\"Image not found: {img_path}\")\n","\n","        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n","        t = torch.from_numpy(img_rgb).permute(2,0,1).contiguous()\n","\n","        t = (t - self.mean_t) / self.std_t\n","        return t\n","\n","    def _load_density_tensor(self, den_path):\n","        den = np.load(den_path).astype(np.float32)\n","        return torch.from_numpy(den).unsqueeze(0)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        part = row['part']\n","        split = row['split']\n","        base_name = Path(row['img_path']).stem\n","\n","        if self.use_saved_tensors and self.tensors_base is not None:\n","            pt_path = self.tensors_base / part / split / (base_name + \".pt\")\n","            if not pt_path.exists():\n","                print(f\"Warning: .pt file not found at {pt_path}. Loading from original files.\")\n","                img_path = row['img_path']\n","                den_path = row['density_full_path'] if self.use_full_density else row['density_path']\n","                img_t = self._load_image_tensor_from_file(img_path)\n","                den_t = self._load_density_tensor(den_path)\n","            else:\n","                d = torch.load(str(pt_path))\n","                img_t = d['image']\n","                den_t = d['density']\n","        else:\n","            img_path = row['img_path']\n","            den_path = row['density_full_path'] if self.use_full_density else row['density_path']\n","            img_t = self._load_image_tensor_from_file(img_path)\n","            den_t = self._load_density_tensor(den_path)\n","\n","        if self.transform:\n","            img_t, den_t = self.transform((img_t, den_t))\n","\n","        return {\"image\": img_t, \"density\": den_t, \"img_path\": str(row['img_path'])}\n","\n","def make_dataloader(manifest_df, batch_size=8, shuffle=True, num_workers=4, use_full_density=False, use_saved_tensors=True, tensors_base=None):\n","    ds = CrowdDatasetTorch(\n","        manifest_df,\n","        use_full_density=use_full_density,\n","        use_saved_tensors=use_saved_tensors,\n","        tensors_base=tensors_base\n","    )\n","    loader = DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","    return loader"],"metadata":{"id":"2qz6i-2crBoO","executionInfo":{"status":"ok","timestamp":1764166478602,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Data Split and DataLoader Initialization\n","\n","This cell splits the manifest into training and testing sets and creates the DataLoaders."],"metadata":{"id":"7XB1ONHSrYZZ"}},{"cell_type":"code","source":["\n","# --- Split Data (Using Part A samples only) ---\n","# Total 300 train samples and 182 test samples will be used.\n","train_df = manifest_df[manifest_df['split'] == 'train_data']\n","test_df = manifest_df[manifest_df['split'] == 'test_data']\n","\n","# --- Create DataLoaders (Optimized for GPU) ---\n","train_loader = make_dataloader(\n","    train_df,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=NUM_WORKERS, # NUM_WORKERS = 4\n","    use_saved_tensors=USE_SAVED_TENSORS,\n","    tensors_base=TENSORS_BASE\n",")\n","\n","test_loader = make_dataloader(\n","    test_df,\n","    batch_size=1, # Keep validation batch size at 1\n","    shuffle=False,\n","    num_workers=NUM_WORKERS,\n","    use_saved_tensors=USE_SAVED_TENSORS,\n","    tensors_base=TENSORS_BASE\n",")\n","\n","print(f\"Training samples (Part A): {len(train_df)}, Batches: {len(train_loader)}\")\n","print(f\"Testing samples (Part A): {len(test_df)}, Batches: {len(test_loader)}\")"],"metadata":{"id":"J2tJ_GbMrZWO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764166494727,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}},"outputId":"1c0f6c36-8cfe-4885-8132-e0696a2e2f42"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples (Part A): 300, Batches: 30\n","Testing samples (Part A): 182, Batches: 182\n"]}]},{"cell_type":"markdown","source":["Model Architecture (VGG-based Regressor)\n","\n","This cell defines the custom Convolutional Neural Network model using a pre-trained VGG-16 backbone."],"metadata":{"id":"0VQ9tpofrxfP"}},{"cell_type":"code","source":["class CrowdCounterModel(nn.Module):\n","    def __init__(self, load_weights=True):\n","        super(CrowdCounterModel, self).__init__()\n","\n","        vgg = vgg16(weights='DEFAULT' if load_weights else None)\n","        features = list(vgg.features.children())\n","\n","        # 1. Feature Extraction (VGG-16 Backbone)\n","        # Block 1-4 (Layers 0:19) are Frozen - Feature stability\n","        self.features_frozen = nn.Sequential(*features[0:19])\n","\n","        # Block 5 (Layers 19:23) is Unfrozen - CRITICAL for fine-tuning deep features\n","        self.features_unfrozen = nn.Sequential(*features[19:23])\n","\n","        # CRITICAL: Freeze only the first four blocks (default weights retained)\n","        for param in self.features_frozen.parameters():\n","            param.requires_grad = False\n","\n","        # 2. Custom Front-end / Regression Layers (Backend)\n","        self.frontend = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 1, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features_frozen(x)\n","        x = self.features_unfrozen(x) # Unfrozen block is now part of the forward pass\n","        x = self.frontend(x)\n","        return x\n","\n","# --- Instantiate Model and Setup Optimizer/Loss ---\n","model = CrowdCounterModel(load_weights=True).to(DEVICE)\n","criterion = nn.MSELoss()\n","\n","# --- CRITICAL: Define Parameter Groups for Fine-Tuning ---\n","# The unfrozen VGG layers need a smaller learning rate (e.g., 1e-7)\n","VGG_LR = 1e-7\n","BASE_LR = 1e-6 # Standard LR for the new layers\n","\n","# Group 1: Parameters for the custom frontend (using standard LR)\n","params_frontend = list(model.frontend.parameters())\n","\n","# Group 2: Parameters for the unfrozen VGG block (using VGG_LR)\n","params_vgg_unfrozen = list(model.features_unfrozen.parameters())\n","\n","# Optimizer now uses differential learning rates for better convergence\n","optimizer = optim.Adam([\n","    {'params': params_frontend, 'lr': BASE_LR},\n","    {'params': params_vgg_unfrozen, 'lr': VGG_LR}\n","])\n","\n","print(\"✅ Model, Optimizer (with Fine-Tuning LR), and Loss function are set up.\")"],"metadata":{"id":"-snu3E6fryrW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764166502087,"user_tz":-330,"elapsed":3689,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}},"outputId":"095e0e80-9b18-425f-d68b-5db5be6e0949"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model, Optimizer (with Fine-Tuning LR), and Loss function are set up.\n"]}]},{"cell_type":"markdown","source":["Evaluation Function (MAE and RMSE)\n","\n","This function will run the model on the test data to calculate the standard crowd counting metrics: MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error)."],"metadata":{"id":"ufXSNcz3sCGd"}},{"cell_type":"code","source":["def evaluate_model(model, data_loader, device):\n","    \"\"\"Calculates MAE and RMSE on the dataset.\"\"\"\n","    model.eval() # Set model to evaluation mode\n","    mae_sum = 0\n","    mse_sum = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n","            images = batch['image'].to(device)\n","            gt_density = batch['density'].to(device)\n","\n","            # Forward pass\n","            pred_density = model(images)\n","\n","            # --- Calculate Predicted and Ground Truth Counts ---\n","            # Sum the entire density map (Hds x Wds) for both prediction and GT\n","            pred_count = torch.sum(pred_density).item()\n","            gt_count = torch.sum(gt_density).item()\n","\n","            # MAE (Mean Absolute Error)\n","            mae_sum += abs(pred_count - gt_count)\n","\n","            # MSE (Mean Squared Error)\n","            mse_sum += (pred_count - gt_count)**2\n","\n","            total_samples += images.size(0)\n","\n","    # Calculate final metrics\n","    final_mae = mae_sum / total_samples\n","    final_rmse = (mse_sum / total_samples)**0.5\n","\n","    return {'mae': final_mae, 'rmse': final_rmse}"],"metadata":{"id":"aie8KHQfsDIf","executionInfo":{"status":"ok","timestamp":1764166509411,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Main Training Loop\n","\n","This is the core training logic where the model is iterated over the training data and validated after each epoch."],"metadata":{"id":"xRiRDH04sLGw"}},{"cell_type":"code","source":["# --- Main Training Loop ---\n","best_mae = float('inf')\n","MODEL_SAVE_NAME = \"best_crowd_counter_model.pth\"\n","MODEL_SAVE_PATH = OUT_DIR / MODEL_SAVE_NAME\n","LOG_BATCH_INTERVAL = 50\n","\n","# Lists to track loss values (Step 4)\n","batch_losses = []\n","epoch_losses = []\n","epoch_val_mae = [] # For visualization\n","epoch_val_rmse = [] # For visualization\n","\n","\n","# --- HELPER FUNCTION: Remap Keys for Fine-Tuning Structure ---\n","def remap_vgg_keys(state_dict):\n","    \"\"\"Converts old continuous VGG keys to the new frozen/unfrozen structure.\"\"\"\n","    new_state_dict = {}\n","    for k, v in state_dict.items():\n","        if k.startswith('features.'):\n","            # Split features based on the new model structure (0:19 is frozen, 19:23 is unfrozen)\n","            layer_index = int(k.split('.')[1])\n","\n","            if layer_index < 19:\n","                # Map to self.features_frozen\n","                new_key = k.replace('features.', 'features_frozen.')\n","                new_key = new_key.replace(f'.{layer_index}.', f'.{layer_index}.') # No need to change index number\n","            else:\n","                # Map to self.features_unfrozen (Layers 19, 21 only—VGG uses 0-indexed features array)\n","                # We need to re-index Block 5 weights: 19 -> 0, 21 -> 2\n","                new_block5_index = layer_index - 19\n","                new_key = k.replace('features.', 'features_unfrozen.')\n","                new_key = new_key.replace(f'.{layer_index}.', f'.{new_block5_index}.')\n","        else:\n","            new_key = k # Frontend/Backend keys remain the same\n","\n","        new_state_dict[new_key] = v\n","    return new_state_dict\n","\n","# --- CRITICAL: Resume Logic and Baseline MAE ---\n","if MODEL_SAVE_PATH.exists():\n","    try:\n","        # Load the raw state dict\n","        old_state_dict = torch.load(str(MODEL_SAVE_PATH), map_location=DEVICE)\n","\n","        # Remap keys from the old structure to the new split structure\n","        remapped_state_dict = remap_vgg_keys(old_state_dict)\n","\n","        # Load the remapped weights into the model\n","        model.load_state_dict(remapped_state_dict)\n","\n","        # Set the correct baseline MAE for competitive saving\n","        best_mae = 134.55\n","\n","        print(f\"✅ Resuming training: Weights remapped and loaded. Baseline MAE set to {best_mae:.2f}.\")\n","\n","    except Exception as e:\n","        print(f\"⚠️ Error loading/remapping previous model weights: {e}. Starting from fresh VGG initialization.\")\n","        best_mae = float('inf')\n","\n","\n","print(\"Starting Training Loop (Validation Enabled)...\")\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    epoch_total_loss = 0\n","\n","    # --- Training Phase ---\n","    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (Train)\")):\n","        images = batch['image'].to(DEVICE)\n","        gt_density = batch['density'].to(DEVICE)\n","\n","        # 1. Get prediction\n","        pred_density = model(images)\n","\n","        # 2. Calculate Loss (Batch Loss)\n","        loss = criterion(pred_density, gt_density)\n","\n","        # 3. Tracking and Verbose Output\n","        batch_losses.append(loss.item())\n","        epoch_total_loss += loss.item()\n","\n","        # Verbose print for Batch Loss\n","        if (i + 1) % LOG_BATCH_INTERVAL == 0:\n","            tqdm.write(f\"VERBOSE BATCH LOSS: {loss.item():.4f}\")\n","\n","        # 4. Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # --- Epoch Summary ---\n","    avg_epoch_loss = epoch_total_loss / len(train_loader)\n","    epoch_losses.append(avg_epoch_loss)\n","\n","    # --- Validation Run ---\n","    eval_metrics = evaluate_model(model, test_loader, DEVICE)\n","    mae = eval_metrics['mae']\n","    rmse = eval_metrics['rmse']\n","\n","    # Store metrics for plotting\n","    epoch_val_mae.append(mae)\n","    epoch_val_rmse.append(rmse)\n","\n","    # --- CRITICAL: Save Best Model Logic ---\n","    save_status = \"\"\n","    if mae < best_mae:\n","        best_mae = mae\n","        torch.save(model.state_dict(), str(MODEL_SAVE_PATH))\n","        save_status = f\" | *** NEW BEST MAE: {best_mae:.2f} (Model Saved) ***\"\n","    else:\n","        save_status = f\" | No improvement (Best: {best_mae:.2f})\"\n","\n","    # --- HORIZONTAL OUTPUT LOGIC ---\n","    print(f\"\\n--- Epoch {epoch+1} Summary ---\"\n","          f\" | TRAIN Loss: {avg_epoch_loss:.4f}\"\n","          f\" | VAL MAE: {mae:.2f}\"\n","          f\" | VAL RMSE: {rmse:.2f}{save_status}\")\n","\n","\n","# --- FINAL POST-TRAINING VISUALIZATION ---\n","print(\"\\n\" + \"=\"*50)\n","print(f\"ALL TRAINING EPOCHS COMPLETE ({NUM_EPOCHS} Epochs).\")\n","plot_training_results(batch_losses, epoch_losses, epoch_val_mae, epoch_val_rmse)\n","print(f\"Best saved MAE across all epochs: {best_mae:.2f}\")\n","print(\"\\nTraining completed.\")"],"metadata":{"id":"FdsLEr89sMA3","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1764167825889,"user_tz":-330,"elapsed":56,"user":{"displayName":"Ishu Rajput","userId":"10570440564969797823"}},"outputId":"e3e73d7c-5181-4ce9-c0f0-9b6e99458b96"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'OUT_DIR' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2462522857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMODEL_SAVE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"best_crowd_counter_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mMODEL_SAVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mMODEL_SAVE_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mLOG_BATCH_INTERVAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'OUT_DIR' is not defined"]}]},{"cell_type":"markdown","source":["Output Visualization."],"metadata":{"id":"tcze0tEte7Mz"}},{"cell_type":"code","source":["# --- New Cell: Visualization Function ---\n","def plot_training_results(batch_losses, epoch_losses, epoch_val_mae, epoch_val_rmse):\n","    \"\"\"\n","    Plots the training loss, epoch loss, and validation metrics (MAE, RMSE)\n","    to analyze the learning pattern.\n","    \"\"\"\n","\n","    # 1. Prepare Epoch Numbers\n","    epochs = range(1, len(epoch_losses) + 1)\n","\n","    plt.figure(figsize=(15, 5))\n","\n","    # --- Plot 1: Training and Epoch Loss ---\n","    plt.subplot(1, 2, 1)\n","\n","    # Plot Epoch Loss (More meaningful trend)\n","    plt.plot(epochs, epoch_losses, 'b', label='Epoch Loss (MSE)')\n","\n","    # Plotting Batch Loss is too dense, so we often stick to Epoch Loss for final plot.\n","\n","    plt.title('Training Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (MSE)')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # --- Plot 2: Validation Metrics (MAE & RMSE) ---\n","    plt.subplot(1, 2, 2)\n","\n","    # Plot Validation MAE\n","    plt.plot(epochs, epoch_val_mae, 'r', label='Validation MAE')\n","\n","    # Plot Validation RMSE\n","    plt.plot(epochs, epoch_val_rmse, 'g', label='Validation RMSE')\n","\n","    plt.title('Validation Metrics (MAE & RMSE)')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Count Error')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Ensure plots are displayed/saved\n","    plt.tight_layout()\n","    plt.savefig(str(OUT_DIR / 'training_results.png'))\n","    plt.show()\n","\n","print(\"Visualization function defined.\")"],"metadata":{"id":"b9qJHCvpfBje"},"execution_count":null,"outputs":[]}]}